import os
import streamlit as st
import time
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain.docstore.document import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

# âœ… API Key ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# âœ… ê²½ë¡œ ì„¤ì •
pdf_path = "C:/Users/USER/Desktop/WebDevelop/Real_Sejong/2026mojip.pdf"
persist_dir = "C:/Users/USER/Desktop/WebDevelop/Real_Sejong/faiss_db"

# âœ… ì§„í•™ì‚¬ URL
jinhak_urls = {
    "2025í•™ë…„ë„ ìˆ˜ì‹œ ê²½ìŸë¥ ": "https://addon.jinhakapply.com/RatioV1/RatioH/Ratio10950551.html",
    "2024í•™ë…„ë„ ìˆ˜ì‹œ ê²½ìŸë¥ ": "https://addon.jinhakapply.com/RatioV1/RatioH/Ratio10950471.html",
    "2023í•™ë…„ë„ ìˆ˜ì‹œ ê²½ìŸë¥ ": "https://addon.jinhakapply.com/RatioV1/RatioH/Ratio10950401.html"
}

# âœ… í¬ë¡¤ë§ í•¨ìˆ˜ (ì…€ë ˆë‹ˆì›€)
def crawl_jinhak_html_selenium(url: str) -> str:
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    service = Service(executable_path="C:/chromedriver-win64/chromedriver.exe")  # ê²½ë¡œ ë§ê²Œ ìˆ˜ì •
    driver = webdriver.Chrome(service=service, options=options)
    driver.get(url)
    time.sleep(2)
    tables = driver.find_elements(By.TAG_NAME, "table")
    all_text = "\n\n".join([table.text for table in tables])
    driver.quit()
    return all_text

# âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± í•¨ìˆ˜
def create_vectorstore():
    all_docs = []

    # 1. PDF ë¡œë”©
    pdf_loader = PyPDFLoader(pdf_path)
    pdf_docs = pdf_loader.load()
    for doc in pdf_docs:
        doc.metadata["source"] = "2026ëª¨ì§‘ìš”ê°• PDF"
    all_docs.extend(pdf_docs)

    # 2. ì§„í•™ì‚¬ í¬ë¡¤ë§
    for label, url in jinhak_urls.items():
        try:
            crawled_text = crawl_jinhak_html_selenium(url)
            all_docs.append(Document(page_content=crawled_text, metadata={"source": label}))
            print(f"ğŸ“„ í¬ë¡¤ë§ëœ {label} í…ìŠ¤íŠ¸ ì¼ë¶€:\n{crawled_text[:300]}")
        except Exception as e:
            print(f"[ERROR] {label} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    # 3. í…ìŠ¤íŠ¸ ë¶„í• 
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=700,
        chunk_overlap=100,
        separators=["\n\n", "\n", ".", " ", ""]
    )
    chunks = splitter.split_documents(all_docs)

    # 4. ë””ë²„ê¹… ë¡œê·¸
    print("âœ… ì €ì¥ëœ ì „ì²´ ì²­í¬ ìˆ˜:", len(chunks))
    for i, chunk in enumerate(chunks):
        if "2025í•™ë…„ë„ ìˆ˜ì‹œ ê²½ìŸë¥ " in chunk.metadata.get("source", ""):
            print(f"\nğŸ” [ì§„í•™ì‚¬ ì²­í¬ ì˜ˆì‹œ] {i+1}ë²ˆ ì²­í¬\n", chunk.page_content[:300])

    # 5. FAISS ë²¡í„°ìŠ¤í† ì–´ ì €ì¥
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
    vectorstore.save_local(persist_dir)
    return vectorstore

# âœ… Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì„¸ì¢…ëŒ€í•™êµ ì…ì‹œ ìƒë‹´ ì±—ë´‡", layout="centered")
st.title("ğŸ“ ì„¸ì¢…ëŒ€í•™êµ ìˆ˜ì‹œ ì…ì‹œ ìƒë‹´ ì±—ë´‡")

# âœ… ë°°ê²½ ì´ë¯¸ì§€ ì„¤ì • í•¨ìˆ˜
import base64

def set_background_image(image_path):
    with open(image_path, "rb") as image_file:
        encoded_string = base64.b64encode(image_file.read()).decode()
    st.markdown(
        f"""
        <style>
        .stApp {{
            background-image: url("data:image/png;base64,{encoded_string}");
            background-size: cover;
            background-repeat: no-repeat;
            background-attachment: fixed;
        }}
        </style>
        """,
        unsafe_allow_html=True
    )

# âœ… ë°°ê²½ ì´ë¯¸ì§€ ì ìš©
background_img_path = "C:/Users/USER/Desktop/WebDevelop/Real_Sejong/RealBack.jpg"
set_background_image(background_img_path)

# âœ… ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ë˜ëŠ” ìƒì„±
embeddings = OpenAIEmbeddings()
if not os.path.exists(persist_dir) or not os.listdir(persist_dir):
    st.info("ğŸ”„ FAISS ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
    db = create_vectorstore()
else:
    db = FAISS.load_local(persist_dir, embeddings, allow_dangerous_deserialization=True)

retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 8})

# âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
system_prompt = """
ë„ˆëŠ” ì„¸ì¢…ëŒ€í•™êµ ì…ì‹œ ì „ë¬¸ ìƒë‹´ ì±—ë´‡ì´ì•¼.
ê²½ìŸë¥ ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì€ ì§„í•™ì‚¬ ë°ì´í„°ì— ê¸°ë°˜í•´ ë‹µë³€í•˜ê³ ,
ê·¸ ì™¸ ì§ˆë¬¸ì€ 2026í•™ë…„ë„ ëª¨ì§‘ìš”ê°• PDFë¥¼ ì°¸ê³ í•´ì„œ ë‹µë³€í•´.
í‘œ, ë¦¬ìŠ¤íŠ¸, êµ¬ì²´ì  ì˜ˆì‹œë¥¼ ì‚¬ìš©í•´ì„œ ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê²Œ ì•Œë ¤ì¤˜.
ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ 'ì…í•™ì²˜ë¡œ ë¬¸ì˜'í•˜ë¼ê³  ì•ˆë‚´í•´.
"""

prompt = PromptTemplate(
    template="{system_prompt}\nì§ˆë¬¸: {question}\nì°¸ê³ ìë£Œ: {context}\në‹µë³€:",
    input_variables=["system_prompt", "question", "context"]
)

llm = ChatOpenAI(model_name="gpt-4o", temperature=0.2)

rag_chain = (
    RunnableParallel({
        "context": retriever,
        "question": RunnablePassthrough()
    }) | prompt.partial(system_prompt=system_prompt)
      | llm
)

# âœ… ë©”ë‰´
menu = st.sidebar.radio(
    "ğŸ”¹ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì„ íƒí•˜ì„¸ìš”",
    [
        "ì „í˜• ì •ë³´ ì•ˆë‚´",
        "ëª¨ì§‘ ìš”ê°• í™•ì¸",
        "ê²½ìŸë¥  ì¡°íšŒ",
        "í•™ê³¼ ì†Œê°œ"
    ]
)

# âœ… ì‚¬ì´ë“œë°” í•˜ë‹¨ ì•ˆë‚´
st.sidebar.markdown("---")
st.sidebar.markdown(
    """
    â„¹ï¸ **ì„¸ì¢…ëŒ€í•™êµ ì…í•™ì²˜**  
    - ì£¼ì†Œ: ì„œìš¸íŠ¹ë³„ì‹œ ê´‘ì§„êµ¬ ëŠ¥ë™ë¡œ 209(êµ°ìë™)  
    - ì „í™”: (02)3408-3456, (02)3408-4455  
    - [ì…í•™ì²˜ í™ˆí˜ì´ì§€ ë°”ë¡œê°€ê¸°](https://ipsi.sejong.ac.kr/)
    """
)

# âœ… ë¡œê³  ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©í•˜ëŠ” í•¨ìˆ˜
def get_base64_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode()

# âœ… ë¡œê³  ê²½ë¡œ
logo_path = "C:/Users/USER/Desktop/WebDevelop/Real_Sejong/logo.png"
encoded_image = get_base64_image(logo_path)

# âœ… ì¤‘ì•™ ì •ë ¬ëœ ë¡œê³  + í…ìŠ¤íŠ¸ ì¶œë ¥
st.sidebar.markdown(
    f"""
    <div style='text-align: center; margin-top: 15px;'>
        <img src='data:image/png;base64,{encoded_image}' width='150'/>
        <p style='margin-top: 5px; font-weight: bold; font-size: 16px;'>ì„¸ì¢…ëŒ€í•™êµ</p>
    </div>
    """,
    unsafe_allow_html=True
)


preset_questions = {
    "ì „í˜• ì •ë³´ ì•ˆë‚´": "í•™ìƒë¶€ ì¢…í•©ì „í˜•ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜.",
    "ëª¨ì§‘ ìš”ê°• í™•ì¸": "ì„¸ì¢…ëŒ€ ìˆ˜ì‹œ ì œì¶œì„œë¥˜ ì•Œë ¤ì¤˜.",
    "ê²½ìŸë¥  ì¡°íšŒ": "2024í•™ë…„ë„ ê²½ì œí•™ê³¼ ê²½ìŸë¥  ì•Œë ¤ì¤˜.",
    "í•™ê³¼ ì†Œê°œ": "ë‚˜ë…¸ì‹ ì†Œì¬ê³µí•™ê³¼ì— ëŒ€í•´ì„œ ì†Œê°œí•´ì¤˜"
}

# âœ… ì§ˆë¬¸ ì…ë ¥
with st.form("question_form"):
    user_query = st.text_area(
        "ê¶ê¸ˆí•œ ì ì„ ì§ì ‘ ì…ë ¥í•˜ê±°ë‚˜, ë©”ë‰´ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”!",
        value=preset_questions[menu],
        height=80
    )
    submitted = st.form_submit_button("ì§ˆë¬¸í•˜ê¸°")

if submitted:
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        #docs = retriever.get_relevant_documents(user_query)
        #for i, d in enumerate(docs):
            #st.write(f"ğŸ”¹ ê²€ìƒ‰ëœ ì²­í¬ {i+1}: ì¶œì²˜ â†’ {d.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
            #st.code(d.page_content[:300])

        answer = rag_chain.invoke(user_query)
        st.markdown(f"#### ë‹µë³€\n{answer.content}" if hasattr(answer, 'content') else answer)
