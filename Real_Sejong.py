import os
from dotenv import load_dotenv
import streamlit as st
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

# .envì—ì„œ OPENAI_API_KEY ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ë²¡í„°ìŠ¤í† ì–´ ë° ë°ì´í„° ê²½ë¡œ
persist_dir = "C:/Users/USER/Desktop/WebDevelop/Real_Sejong/chroma_db"
data_path = "C:/Users/USER/Desktop/WebDevelop/Real_Sejong/data.txt"

st.set_page_config(page_title="ì„¸ì¢…ëŒ€í•™êµ ì…ì‹œ ìƒë‹´ ì±—ë´‡", layout="centered")
st.title("ğŸ“ ì„¸ì¢…ëŒ€í•™êµ ìˆ˜ì‹œ ì…ì‹œ ìƒë‹´ ì±—ë´‡")

embeddings = OpenAIEmbeddings()

def create_vectorstore():
    with open(data_path, 'r', encoding='utf-8') as f:
        data = f.read()
    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=80)
    docs = splitter.create_documents([data])
    vectorstore = Chroma.from_documents(
        docs,
        persist_directory=persist_dir
    )
    vectorstore.persist()
    return vectorstore

# ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ/ìƒì„±
if not os.path.exists(persist_dir) or len(os.listdir(persist_dir)) == 0:
    st.info("ğŸ”„ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
    db = create_vectorstore()
else:
    db = Chroma(persist_directory=persist_dir, embedding_function=embeddings)   # â˜…â˜…â˜… ì¶”ê°€!!

retriever = db.as_retriever(search_kwargs={"k": 5})

# í”„ë¡¬í”„íŠ¸ ì •ì˜
system_prompt = """
ë„ˆëŠ” ì„¸ì¢…ëŒ€í•™êµ ì…ì‹œ ì „ë¬¸ ìƒë‹´ ì±—ë´‡ì´ì•¼.
ì•„ë˜ ì •ë³´ì— ê¸°ë°˜í•´ ì‚¬ìš©ì ì§ˆë¬¸ì— ì„±ì‹¤í•˜ê³  ì •í™•í•˜ê²Œ, í‘œë‚˜ ë¦¬ìŠ¤íŠ¸ë¡œ ê¹”ë”í•˜ê²Œ ì•ˆë‚´í•´ì¤˜.
ë¶ˆí™•ì‹¤í•œ ê²½ìš° 'ì…í•™ì²˜ë¡œ ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤'ë¼ê³  ì•ˆë‚´í•´.
"""

prompt = PromptTemplate(
    template="{system_prompt}\nì§ˆë¬¸: {question}\nì°¸ê³ ìë£Œ: {context}\në‹µë³€:",
    input_variables=["system_prompt", "question", "context"]
)

llm = ChatOpenAI(model_name="gpt-4o", temperature=0.2)

# LCEL ì²´ì¸ êµ¬ì„±
rag_chain = (
    RunnableParallel({
        "context": retriever,
        "question": RunnablePassthrough()
    })
    | prompt.partial(system_prompt=system_prompt)
    | llm
)

# ë©”ë‰´ ë²„íŠ¼
menu = st.sidebar.radio(
    "ğŸ”¹ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì„ íƒí•˜ì„¸ìš”",
    [
        "ì „í˜• ì •ë³´ ì•ˆë‚´",
        "ëª¨ì§‘ ìš”ê°• í™•ì¸",
        "ê²½ìŸë¥ /ì»·íŠ¸ë¼ì¸ ì¡°íšŒ",
        "í•™ê³¼ ì†Œê°œ",
        "ê³¼ê±° ë§ˆì§€ë§‰ í•©ê²©/ì˜ˆë¹„ë²ˆí˜¸"
    ]
)

preset_questions = {
    "ì „í˜• ì •ë³´ ì•ˆë‚´": "ì„¸ì¢…ëŒ€í•™êµ í•™ìƒë¶€ ì¢…í•©ì „í˜•, êµê³¼ì „í˜•, ë…¼ìˆ ì „í˜•ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜.",
    "ëª¨ì§‘ ìš”ê°• í™•ì¸": "ì„¸ì¢…ëŒ€ í•™ê³¼ë³„ ëª¨ì§‘ì¸ì›, ì§€ì›ìê²©, ì œì¶œì„œë¥˜ ì•Œë ¤ì¤˜.",
    "ê²½ìŸë¥ /ì»·íŠ¸ë¼ì¸ ì¡°íšŒ": "ìµœê·¼ 3ë…„ê°„ ì„¸ì¢…ëŒ€ ê²½ìŸë¥ ê³¼ ì»·íŠ¸ë¼ì¸ ì•Œë ¤ì¤˜.",
    "í•™ê³¼ ì†Œê°œ": "ì„¸ì¢…ëŒ€í•™êµ ê° í•™ê³¼ë¥¼ ê°„ëµíˆ ì†Œê°œí•´ì¤˜.",
    "ê³¼ê±° ë§ˆì§€ë§‰ í•©ê²©/ì˜ˆë¹„ë²ˆí˜¸": "2023~2024í•™ë…„ë„ ë§ˆì§€ë§‰í•©ê²©ì ì˜ˆë¹„ë²ˆí˜¸ ì•Œë ¤ì¤˜"
}

with st.form("question_form"):
    user_query = st.text_area(
        "ê¶ê¸ˆí•œ ì ì„ ì§ì ‘ ì…ë ¥í•˜ê±°ë‚˜, ë©”ë‰´ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”!",
        value=preset_questions[menu],
        height=80
    )
    submitted = st.form_submit_button("ì§ˆë¬¸í•˜ê¸°")

if submitted:
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        answer = rag_chain.invoke(user_query)
        st.markdown(f"#### ë‹µë³€\n{answer.content}" if hasattr(answer, 'content') else answer)

st.markdown("---")
st.markdown(
    """
    â„¹ï¸ **ì„¸ì¢…ëŒ€í•™êµ ì…í•™ì²˜**  
    - ì£¼ì†Œ: ì„œìš¸íŠ¹ë³„ì‹œ ê´‘ì§„êµ¬ ëŠ¥ë™ë¡œ 209(êµ°ìë™), ì„¸ì¢…ëŒ€í•™êµ ì…í•™ì²˜  
    - ìš°í¸ë²ˆí˜¸: 02006  
    - ì „í™”: (02)3408-3456, (02)3408-4455  
    - íŒ©ìŠ¤: (02)3408-3556  
    - [ì…í•™ì²˜ í™ˆí˜ì´ì§€ ë°”ë¡œê°€ê¸°](https://ipsi.sejong.ac.kr/)
    """
)
